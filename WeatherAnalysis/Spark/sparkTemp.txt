//How to return list of files in a directory
new java.io.File(dirName).listFiles.filter(_.getName.endsWith(".txt"))


import java.io.File
//PWD object
val pwd = System.getProperty("user.dir")
//Module definition
def getListOfFiles(dir: String):List[File] = {
    val d = new File(dir)
    if (d.exists && d.isDirectory) {
        d.listFiles.filter(_.isFile).toList
    } else {
        List[File]()
    }
}
//Or If you are sure the parameter is a directory
def getListOfFiles(dir: File):List[File] = dir.listFiles.filter(_.isFile).toList
//List PWD files
 val files = getListOfFiles(pwd)


//Changing Directroies

scala> System.getProperty("user.dir")
res1: java.lang.String = /Users/dare.olufunmilayo
scala> System.setProperty("user.dir", "/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath")



//Working in Scala Spark REPL
import breeze.linalg._
import breeze.plot._
import breeze.numerics._
import org.apache..spark.sql.types.DataTypes 
import org.apache..spark.sql.types.StructField
import org.apache..spark.sql.types.StructType
import org.apache..spark.sql.types.SQLContext

//Loading REPL with csv module

//Configuration

val sparkConf = new SparkConf().setAppName("sparkSQLExamples").setMaster("local[*]")
    .setIfMissing("hive.execution.engine", "spark")
val sparkContext = new SparkContext(sparkConf)
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)


spark-shell --packages com.databricks:spark-csv_2.10:1.3.0

val firstData = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").option("inferSChema", "true").load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1901.csv")

//HDFS Version
val firstData = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("hdfs:///user/dare.olufunmilayo/WeatherData/029070-99999-1901.csv")

//Write to Hive  -- This is writing to HDFS, i need to figure out how to write into Hive table
firstData.write.format("parquet").save("/user/dare.olufunmilayo/WeatherData/Transformed_to_parquet/sample_1901")

//TO use custom schema

import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType}

//To write data to Hive

import org.apache.spark.sql.SaveMode

val customSchema = StructType (Array(
				   StructField("Year", IntegerType, true),
				   StructField("Month", IntegerType, true),
				   StructField("Day", IntegerType, true),
				   StructField("Hour", IntegerType, true),
				   StructField("Temp", IntegerType, true),
				   StructField("DewTemp", IntegerType, true),
				   StructField("Pressure", IntegerType, true),
				   StructField("WinDir", IntegerType, true),
				   StructField("WinSpeed", IntegerType, true),
				   StructField("Sky", IntegerType, true),
				   StructField("Precipitation1", IntegerType, true),
				   StructField("Precipitation6", IntegerType, true),
				   StructField("StationID", StringType, true)
)

)



val firstyear = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1901.csv")

firstyear.registerTempTable("year1")

val sample = sqlContext.sql("select  Year, Month, Day from year1 limit 5")

sample.collect.foreach(println)

sample.printSchema

//Load second year

val secondyear = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1902.csv")


secondyear.registerTempTable("year2")




//Load third year

val thirdyear = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1903.csv")


thirdyear.registerTempTable("year3")




//Load fourth year

val fourthyear = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1904.csv")


fourthyear.registerTempTable("year4")



//Load fifth year

val fifthyear = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(customSchema).load("/Users/dare.olufunmilayo/GIT/Scripts/Python/downloads/extracts/csvPath/029070-99999-1905.csv")


fifthyear.registerTempTable("year5")

//Join all the tables together using a union, and add an extra column to concatenate the dates as one
val allyears= year1RDD.unionAll(year2RDD).unionAll(year3RDD).unionAll(year4RDD).unionAll(year5RDD)

allyears.registerTempTable("WD")

//Define data analysis to be done
--count all the rows


//transform the data into Parquet
//Load the data into hive as parquet
//connect Tableau to hive


val sample = sqlContext.sql("select  * from WD-1901-1905 limit 10")

sample.collect.foreach(println)

sample.printSchema



Storing to Hive
create database support
use support;
create external table wd_1901_1905 (Year int, Month int , Day int, Hour int, Temp int, DewTemp int, Pressure int , WinDir int,  WinSpeed int, Sky int,  Precipitation1 int, Precipitation6 int, StationID string)
stored as Parquet
location  '/user/dare.olufunmilayo/WeatherData/Transformed_to_Parquet'


























